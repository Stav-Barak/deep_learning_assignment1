{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "nfSr2JJzFOgn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNnOTK2DP1Xu"
      },
      "source": [
        "## Part 1: Forward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "ntEffimkNP9G"
      },
      "outputs": [],
      "source": [
        "np.random.seed(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "Kmv8ZHH3GVGv"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(layer_dims):\n",
        "    \"\"\"\n",
        "    Initialize the parameters W and b for each layer in the network.\n",
        "\n",
        "    Arguments:\n",
        "    layer_dims -- list containing the dimensions of each layer in the network\n",
        "\n",
        "    Returns:\n",
        "    parameters -- dictionary containing initialized W and b parameters\n",
        "    \"\"\"\n",
        "    parameters = {}\n",
        "    for l in range(1, len(layer_dims)):\n",
        "        parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], layer_dims[l - 1]) / layer_dims[l-1]\n",
        "        parameters[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n",
        "    return parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "lPOwQrArL3du"
      },
      "outputs": [],
      "source": [
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Compute the linear part of a layer's forward propagation.\n",
        "\n",
        "    Arguments:\n",
        "    A -- activations from the previous layer\n",
        "    W -- weight matrix of the current layer\n",
        "    b -- bias vector of the current layer\n",
        "\n",
        "    Returns:\n",
        "    Z -- linear component of the activation function\n",
        "    linear_cache -- dictionary containing A, W, b\n",
        "    \"\"\"\n",
        "    Z = np.dot(W, A) + b\n",
        "    linear_cache = {\"A\": A, \"W\": W, \"b\": b}\n",
        "    return Z, linear_cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "QKEPfyckNBX3"
      },
      "outputs": [],
      "source": [
        "def softmax(Z):\n",
        "    \"\"\"\n",
        "    Compute the softmax activation for the input Z.\n",
        "\n",
        "    Arguments:\n",
        "    Z -- linear component of the activation function\n",
        "\n",
        "    Returns:\n",
        "    A -- activations of the layer\n",
        "    activation_cache -- returns Z for use in backpropagation\n",
        "    \"\"\"\n",
        "    Z = Z - np.max(Z, axis=0, keepdims=True)  # Normalize Z\n",
        "    A = np.exp(Z) / np.sum(np.exp(Z), axis=0, keepdims=True)\n",
        "    return A, Z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "NmuIEklnNVfa"
      },
      "outputs": [],
      "source": [
        "def relu(Z):\n",
        "    \"\"\"\n",
        "    Compute the ReLU activation for the input Z.\n",
        "\n",
        "    Arguments:\n",
        "    Z -- linear component of the activation function\n",
        "\n",
        "    Returns:\n",
        "    A -- activations of the layer\n",
        "    activation_cache -- returns Z for use in backpropagation\n",
        "    \"\"\"\n",
        "    A = np.maximum(0, Z)\n",
        "    return A, Z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "EGiAviTQOVrG"
      },
      "outputs": [],
      "source": [
        "def linear_activation_forward(A_prev, W, B, activation):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation step for the LINEAR->ACTIVATION layer.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    A_prev : np.ndarray\n",
        "        Activations from the previous layer, of shape (size of previous layer, number of examples).\n",
        "    W : np.ndarray\n",
        "        Weights matrix of the current layer, of shape (size of current layer, size of previous layer).\n",
        "    B : np.ndarray\n",
        "        Bias vector of the current layer, of shape (size of current layer, 1).\n",
        "    activation : str\n",
        "        The activation function to be applied. Must be either \"relu\" or \"softmax\".\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    A : np.ndarray\n",
        "        The post-activation value, of shape (size of current layer, number of examples).\n",
        "    cache : dict\n",
        "        A dictionary containing:\n",
        "        - \"linear_cache\": A dictionary with values (A_prev, W, B) for use in backward propagation.\n",
        "        - \"activation_cache\": Z (the input to the activation function) for use in backward propagation.\n",
        "\n",
        "    Raises:\n",
        "    -------\n",
        "    Exception:\n",
        "        If the provided activation function is not \"relu\" or \"softmax\".\n",
        "\n",
        "    \"\"\"\n",
        "    # Compute linear part of forward propagation\n",
        "    Z, linear_cache = linear_forward(A_prev, W, B)\n",
        "\n",
        "    # Apply the specified activation function\n",
        "    if activation == 'relu':\n",
        "        A, activation_cache = relu(Z)\n",
        "    elif activation == 'softmax':\n",
        "        A, activation_cache = softmax(Z)\n",
        "    else:\n",
        "        raise Exception('Activation function must be either \"relu\" or \"softmax\"')\n",
        "\n",
        "    # Combine linear and activation caches\n",
        "    cache = {\"linear_cache\": linear_cache, \"activation_cache\": activation_cache}\n",
        "\n",
        "    return A, cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "xs1_EUTSPwCS"
      },
      "outputs": [],
      "source": [
        "def l_model_forward(X, parameters, use_batchnorm=False):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX.\n",
        "\n",
        "    Arguments:\n",
        "    X -- input data\n",
        "    parameters -- initialized W and b parameters\n",
        "    use_batchnorm -- flag to determine whether to apply batch normalization\n",
        "\n",
        "    Returns:\n",
        "    AL -- output of the last layer\n",
        "    caches -- list of caches for each layer\n",
        "    \"\"\"\n",
        "    A = X\n",
        "    caches = []\n",
        "    L = len(parameters) // 2\n",
        "\n",
        "    for l in range(1, L):\n",
        "        A_prev = A\n",
        "        A, cache = linear_activation_forward(A_prev, parameters[f\"W{l}\"], parameters[f\"b{l}\"], \"relu\")\n",
        "        if use_batchnorm:\n",
        "            A = apply_batchnorm(A)\n",
        "        caches.append(cache)\n",
        "\n",
        "    AL, cache = linear_activation_forward(A, parameters[f\"W{L}\"], parameters[f\"b{L}\"], \"softmax\")\n",
        "    caches.append(cache)\n",
        "\n",
        "    return AL, caches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "SZm9ciaMTigI"
      },
      "outputs": [],
      "source": [
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Compute the categorical cross-entropy cost.\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector from softmax, shape (num_classes, number of examples)\n",
        "    Y -- ground truth labels, one-hot encoded, shape (num_classes, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \"\"\"\n",
        "    m = Y.shape[1]\n",
        "    cost = -np.sum(Y * np.log(AL + 1e-8)) / (m+1e-8)  # Added epsilon for numerical stability\n",
        "    return cost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "9HqdMh3mRTQu"
      },
      "outputs": [],
      "source": [
        "def apply_batchnorm(A):\n",
        "    \"\"\"\n",
        "    Perform batch normalization on the activations.\n",
        "\n",
        "    Arguments:\n",
        "    A -- activations of a given layer\n",
        "\n",
        "    Returns:\n",
        "    NA -- normalized activations\n",
        "    \"\"\"\n",
        "    mean = np.mean(A, axis=1, keepdims=True)\n",
        "    variance = np.var(A, axis=1, keepdims=True)\n",
        "    NA = (A - mean) / np.sqrt(variance + 1e-8)  # Added epsilon for stability\n",
        "    return NA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jm8gzEEW7Vs"
      },
      "source": [
        "# Part 2: Backward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "ckAvDG8PXAh0"
      },
      "outputs": [],
      "source": [
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implements the linear portion of backward propagation for a single layer.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    dZ : numpy.ndarray\n",
        "        Gradient of the cost with respect to the linear output of the current layer.\n",
        "    cache : dict\n",
        "        Dictionary containing:\n",
        "        - \"A\": Activations from the previous layer.\n",
        "        - \"W\": Weights of the current layer.\n",
        "        - \"b\": Biases of the current layer.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dA_prev : numpy.ndarray\n",
        "        Gradient of the cost with respect to the activations of the previous layer.\n",
        "    dW : numpy.ndarray\n",
        "        Gradient of the cost with respect to the weights of the current layer.\n",
        "    db : numpy.ndarray\n",
        "        Gradient of the cost with respect to the biases of the current layer.\n",
        "    \"\"\"\n",
        "    # Extract cached values\n",
        "    A_prev = cache[\"A\"]\n",
        "    W = cache[\"W\"]\n",
        "    m = A_prev.shape[1]  # Number of examples\n",
        "\n",
        "    # Compute gradients\n",
        "    dW = np.dot(dZ, A_prev.T) / m\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "\n",
        "    return dA_prev, dW, db\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "MtByTVf4fFug"
      },
      "outputs": [],
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \"\"\"\n",
        "    Implements the backward propagation step for the LINEAR->ACTIVATION layer.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    dA : np.ndarray\n",
        "        Gradient of the cost with respect to the post-activation value,\n",
        "        of shape (size of current layer, number of examples).\n",
        "    cache : dict\n",
        "        Dictionary containing:\n",
        "        - \"linear_cache\": Cached values (A_prev, W, B) from the forward propagation step.\n",
        "        - \"activation_cache\": Cached Z value (linear activation input) from the forward propagation step.\n",
        "    activation : str\n",
        "        The activation function used in the forward propagation. Must be either \"relu\" or \"softmax\".\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dA_prev : np.ndarray\n",
        "        Gradient of the cost with respect to the activation of the previous layer,\n",
        "        of shape (size of previous layer, number of examples).\n",
        "    dW : np.ndarray\n",
        "        Gradient of the cost with respect to the weights of the current layer,\n",
        "        of shape (size of current layer, size of previous layer).\n",
        "    db : np.ndarray\n",
        "        Gradient of the cost with respect to the biases of the current layer,\n",
        "        of shape (size of current layer, 1).\n",
        "\n",
        "    Raises:\n",
        "    -------\n",
        "    Exception:\n",
        "        If the provided activation function is not \"relu\" or \"softmax\".\n",
        "    \"\"\"\n",
        "    # Retrieve cached values\n",
        "    linear_cache = cache[\"linear_cache\"]\n",
        "    activation_cache = cache[\"activation_cache\"]\n",
        "\n",
        "    # Compute the gradient of Z based on the activation function\n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "    elif activation == \"softmax\":\n",
        "        dZ = softmax_backward(dA, activation_cache)\n",
        "    else:\n",
        "        raise Exception('Activation function must be either \"relu\" or \"softmax\"')\n",
        "\n",
        "    # Compute gradients for the linear component\n",
        "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "    return dA_prev, dW, db\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "RYNQ2fVUi3pg"
      },
      "outputs": [],
      "source": [
        "def relu_backward(dA, activation_cache):\n",
        "    \"\"\"\n",
        "    Implements the backward propagation for a ReLU activation.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    dA : numpy.ndarray\n",
        "        Gradient of the cost with respect to the activation of the current layer.\n",
        "    activation_cache : numpy.ndarray\n",
        "        Stored linear activation (Z) from the forward propagation.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dZ : numpy.ndarray\n",
        "        Gradient of the cost with respect to Z (input to ReLU).\n",
        "    \"\"\"\n",
        "    # Retrieve Z from the activation cache\n",
        "    Z = activation_cache\n",
        "\n",
        "    # Initialize dZ as a copy of dA\n",
        "    dZ = dA * (Z > 0).astype(float)\n",
        "\n",
        "    return dZ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "k3aFwt7Jjz8h"
      },
      "outputs": [],
      "source": [
        "def softmax_backward(dA, activation_cache):\n",
        "    \"\"\"\n",
        "    Implements the backward propagation for a softmax activation.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    dA : numpy.ndarray\n",
        "        Gradient of the cost with respect to the activation of the current layer.\n",
        "    activation_cache : dict\n",
        "        Dictionary containing:\n",
        "        - \"Y\": True labels (one-hot encoded).\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dZ : numpy.ndarray\n",
        "        Gradient of the cost with respect to Z (input to softmax).\n",
        "    \"\"\"\n",
        "    # Extract true labels from the activation cache\n",
        "    Y_true = activation_cache[\"Y\"]\n",
        "\n",
        "    # Compute gradient\n",
        "    dZ = dA - Y_true\n",
        "\n",
        "    return dZ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "nkaoU_-Bkyv1"
      },
      "outputs": [],
      "source": [
        "def l_model_backward(AL, Y, caches):\n",
        "    \"\"\"\n",
        "    Implements the backward propagation for the entire network.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    AL : numpy.ndarray\n",
        "        Post-activation output of the final layer.\n",
        "    Y : numpy.ndarray\n",
        "        True labels in one-hot encoded format.\n",
        "    caches : list\n",
        "        List of caches for each layer, generated during forward propagation.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    grads : dict\n",
        "        Dictionary containing gradients for all layers.\n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches)  # Number of layers in the model\n",
        "\n",
        "    # Backpropagation for the final layer (Softmax)\n",
        "    final_cache = caches[-1]\n",
        "    softmax_cache = {\n",
        "        \"linear_cache\": final_cache[\"linear_cache\"],\n",
        "        \"activation_cache\": {\"Z\": final_cache[\"activation_cache\"], \"Y\": Y},\n",
        "    }\n",
        "    dA_prev, dW, db = linear_activation_backward(AL, softmax_cache, activation=\"softmax\")\n",
        "    grads[f\"dA{L}\"] = dA_prev\n",
        "    grads[f\"dW{L}\"] = dW\n",
        "    grads[f\"db{L}\"] = db\n",
        "\n",
        "    # Backpropagation for hidden layers (ReLU)\n",
        "    for l in reversed(range(1, L)):  # From L-1 to 1\n",
        "        current_cache = caches[l - 1]\n",
        "        dA_prev, dW, db = linear_activation_backward(dA_prev, current_cache, activation=\"relu\")\n",
        "        grads[f\"dA{l}\"] = dA_prev\n",
        "        grads[f\"dW{l}\"] = dW\n",
        "        grads[f\"db{l}\"] = db\n",
        "\n",
        "    return grads\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "WfgZucblrgcd"
      },
      "outputs": [],
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Updates parameters using gradient descent.\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- Dictionary containing the DNN parameters\n",
        "    grads -- Dictionary containing the gradients (from L_model_backward)\n",
        "    learning_rate -- Learning rate for gradient descent\n",
        "\n",
        "    Returns:\n",
        "    parameters -- Updated parameters\n",
        "    \"\"\"\n",
        "    L = len(parameters) // 2  # Number of layers\n",
        "\n",
        "    for l in range(1, L + 1):\n",
        "        parameters[f\"W{l}\"] -= learning_rate * grads[f\"dW{l}\"]\n",
        "        parameters[f\"b{l}\"] -= learning_rate * grads[f\"db{l}\"]\n",
        "\n",
        "    return parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCA0bTLis8Ne"
      },
      "source": [
        "# Part 3: Train the network and produce predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "XvrHPcwns76M"
      },
      "outputs": [],
      "source": [
        "def l_layer_model(X, Y, layers_dims, learning_rate, num_iterations=100, batch_size=256, use_bnorm=False):\n",
        "    \"\"\"\n",
        "    Trains a deep neural network with optional batch normalization.\n",
        "\n",
        "    Parameters:\n",
        "    ---------\n",
        "    X : np.ndarray\n",
        "        Input data with shape (features, number_of_samples).\n",
        "    Y : np.ndarray\n",
        "        True labels (one-hot encoded) with shape (classes, number_of_samples).\n",
        "    layers_dims : list\n",
        "        List specifying the size of each layer in the network.\n",
        "    learning_rate : float\n",
        "        Learning rate for the gradient descent updates.\n",
        "    num_iterations : int, optional\n",
        "        Total number of epochs for training. Default is 100.\n",
        "    batch_size : int, optional\n",
        "        Number of samples per mini-batch. Default is 32.\n",
        "    use_bnorm : bool, optional\n",
        "        If True, apply batch normalization during training.\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    parameters : dict\n",
        "        The learned weights and biases of the network.\n",
        "    costs_train : list\n",
        "        Cost values recorded at checkpoints during training.\n",
        "    \"\"\"\n",
        "    # Lists to track costs for training and validation\n",
        "    costs_train = []\n",
        "    costs_valid = []\n",
        "\n",
        "    # Split data into training (80%) and validation (20%)\n",
        "    num_samples = X.shape[1]\n",
        "    train_size = int(0.8 * num_samples)\n",
        "    indices = np.random.permutation(num_samples)\n",
        "    train_indices = indices[:train_size]\n",
        "    valid_indices = indices[train_size:]\n",
        "\n",
        "    X_train, X_valid = X[:, train_indices], X[:, valid_indices]\n",
        "    Y_train, Y_valid = Y[:, train_indices], Y[:, valid_indices]\n",
        "\n",
        "    # Initialize parameters for the network\n",
        "    parameters = initialize_parameters(layers_dims)\n",
        "\n",
        "    # Counters for iterations and epochs\n",
        "    batch_counter = 1\n",
        "    total_epochs = 0\n",
        "\n",
        "    # Training loop over epochs\n",
        "    for epoch in range(num_iterations):\n",
        "        # Mini-batch loop\n",
        "        for start in range(0, X_train.shape[1], batch_size):\n",
        "            end = min(start + batch_size, X_train.shape[1])\n",
        "            X_batch = X_train[:, start:end]\n",
        "            Y_batch = Y_train[:, start:end]\n",
        "\n",
        "            # Forward pass\n",
        "            AL, caches = l_model_forward(X_batch, parameters, use_bnorm)\n",
        "\n",
        "            # Backward pass\n",
        "            gradients = l_model_backward(AL, Y_batch, caches)\n",
        "\n",
        "            # Update parameters\n",
        "            parameters = update_parameters(parameters, gradients, learning_rate)\n",
        "\n",
        "            # Track the epoch number\n",
        "            total_epochs = epoch\n",
        "\n",
        "            # Periodic cost logging\n",
        "            if batch_counter % 100 == 0:\n",
        "                # Compute training cost\n",
        "                train_cost = compute_cost(AL, Y_batch)\n",
        "                costs_train.append(train_cost)\n",
        "\n",
        "                # Compute validation cost\n",
        "                AL_valid, _ = l_model_forward(X_valid, parameters, use_bnorm)\n",
        "                valid_cost = compute_cost(AL_valid, Y_valid)\n",
        "                costs_valid.append(valid_cost)\n",
        "\n",
        "                print(f\"Step {batch_counter}: Training Cost = {train_cost:.5f}, Validation Cost = {valid_cost:.5f}\")\n",
        "\n",
        "                # Early stopping: Check validation cost improvement\n",
        "                if len(costs_valid) > 1 and abs(costs_valid[-2] - costs_valid[-1]) < 0.0001:\n",
        "                    validation_accuracy = predict(X_valid, Y_valid, parameters, use_bnorm)\n",
        "                    print(\"Early stopping activated.\")\n",
        "                    print(f\"Final Training Cost: {train_cost}\")\n",
        "                    print(f\"Final Validation Cost: {valid_cost}\")\n",
        "                    print(f\"Accuracy on validation: {validation_accuracy}\")\n",
        "                    print(f\"Epoch: {total_epochs}, Iterations: {batch_counter}\")\n",
        "\n",
        "\n",
        "                    # Compute average magnitudes of weights for each layer\n",
        "                    print(\"\\nAverage Magnitude of Weights ('without' L2):\")\n",
        "                    print(f\"This run is {'with' if use_bnorm else 'without'} batch normalization. (use_bnorm={use_bnorm})\")\n",
        "                    for l in range(1, len(layers_dims)):\n",
        "                        W = parameters[f\"W{l}\"]\n",
        "                        avg_magnitude = np.mean(np.abs(W))\n",
        "                        print(f\"Layer {l}: Average Magnitude = {avg_magnitude:.5f}\")\n",
        "\n",
        "                    return parameters, costs_train\n",
        "\n",
        "            batch_counter += 1\n",
        "\n",
        "    # Final cost computations\n",
        "    final_train_cost = compute_cost(AL, Y_batch)\n",
        "    final_valid_cost = compute_cost(AL_valid, Y_valid)\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "    print(f\"Final Training Cost: {final_train_cost}\")\n",
        "    print(f\"Final Validation Cost: {final_valid_cost}\")\n",
        "    print(\"Accuracy on validation: \", predict(X_valid, Y_valid, parameters, use_bnorm))\n",
        "    print(f\"Epoch: {total_epochs}, Iterations: {batch_counter}\")\n",
        "\n",
        "\n",
        "    # Compute average magnitudes of weights for each layer\n",
        "    print(\"\\nAverage Magnitude of Weights (without L2):\")\n",
        "    print(f\"This run is {'with' if use_bnorm else 'without'} batch normalization. (use_bnorm={use_bnorm})\")\n",
        "    for l in range(1, len(layers_dims)):\n",
        "        W = parameters[f\"W{l}\"]\n",
        "        avg_magnitude = np.mean(np.abs(W))\n",
        "        print(f\"Layer {l}: Average Magnitude = {avg_magnitude:.5f}\")\n",
        "\n",
        "    return parameters, costs_train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "CZxtA4D7saf2"
      },
      "outputs": [],
      "source": [
        "def predict(X, Y, parameters,use_bnorm= False):\n",
        "    \"\"\"\n",
        "    Predict the labels for the input data and calculate accuracy.\n",
        "\n",
        "    Arguments:\n",
        "    X -- Input data, numpy array of shape (height*width, number_of_examples)\n",
        "    Y -- True labels of the data, one-hot encoded, shape (num_of_classes, number_of_examples)\n",
        "    parameters -- Dictionary containing the DNN architecture's parameters\n",
        "\n",
        "    Returns:\n",
        "    accuracy -- Accuracy of the model as a percentage\n",
        "    \"\"\"\n",
        "    # Forward propagation\n",
        "    AL, _ = l_model_forward(X, parameters, use_batchnorm=use_bnorm)\n",
        "\n",
        "    # Predictions: Choose the class with the highest probability\n",
        "    predictions = np.argmax(AL, axis=0)  # Predicted class indices\n",
        "    true_labels = np.argmax(Y, axis=0)  # True class indices\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = np.mean(predictions == true_labels)\n",
        "\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0msq__e8ucGw"
      },
      "source": [
        "# MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "wh02Ij5ZubUA"
      },
      "outputs": [],
      "source": [
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values to the range [0, 1]\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Flatten the images (convert 2D arrays into 1D arrays)\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)  # Flatten each image\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)    # Flatten each image\n",
        "\n",
        "# Convert labels to one-hot encoded format\n",
        "num_classes = 10\n",
        "y_train_one_hot = np.eye(num_classes)[y_train]  # One-hot encoding for training labels\n",
        "y_test_one_hot = np.eye(num_classes)[y_test]    # One-hot encoding for test labels\n",
        "\n",
        "# Assign the one-hot encoded labels back to the original variables\n",
        "y_train = y_train_one_hot\n",
        "y_test = y_test_one_hot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wl6BVaw1p3mE",
        "outputId": "9063e784-9f39-4ebe-be34-51d9db987638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100: Training Cost = 2.30193, Validation Cost = 2.30221\n",
            "Step 200: Training Cost = 2.30199, Validation Cost = 2.30168\n",
            "Step 300: Training Cost = 2.29946, Validation Cost = 2.30100\n",
            "Step 400: Training Cost = 2.30110, Validation Cost = 2.30024\n",
            "Step 500: Training Cost = 2.29900, Validation Cost = 2.29941\n",
            "Step 600: Training Cost = 2.30050, Validation Cost = 2.29857\n",
            "Step 700: Training Cost = 2.29997, Validation Cost = 2.29753\n",
            "Step 800: Training Cost = 2.29411, Validation Cost = 2.29640\n",
            "Step 900: Training Cost = 2.29421, Validation Cost = 2.29502\n",
            "Step 1000: Training Cost = 2.29466, Validation Cost = 2.29339\n",
            "Step 1100: Training Cost = 2.28864, Validation Cost = 2.29133\n",
            "Step 1200: Training Cost = 2.28832, Validation Cost = 2.28870\n",
            "Step 1300: Training Cost = 2.28311, Validation Cost = 2.28523\n",
            "Step 1400: Training Cost = 2.28001, Validation Cost = 2.28064\n",
            "Step 1500: Training Cost = 2.27407, Validation Cost = 2.27427\n",
            "Step 1600: Training Cost = 2.26511, Validation Cost = 2.26523\n",
            "Step 1700: Training Cost = 2.25656, Validation Cost = 2.25186\n",
            "Step 1800: Training Cost = 2.23845, Validation Cost = 2.23114\n",
            "Step 1900: Training Cost = 2.20686, Validation Cost = 2.19846\n",
            "Step 2000: Training Cost = 2.14813, Validation Cost = 2.14524\n",
            "Step 2100: Training Cost = 2.08421, Validation Cost = 2.06977\n",
            "Step 2200: Training Cost = 2.00294, Validation Cost = 1.98401\n",
            "Step 2300: Training Cost = 1.90999, Validation Cost = 1.91349\n",
            "Step 2400: Training Cost = 1.87754, Validation Cost = 1.85980\n",
            "Step 2500: Training Cost = 1.81329, Validation Cost = 1.81742\n",
            "Step 2600: Training Cost = 1.81290, Validation Cost = 1.78201\n",
            "Step 2700: Training Cost = 1.75978, Validation Cost = 1.75010\n",
            "Step 2800: Training Cost = 1.78022, Validation Cost = 1.71908\n",
            "Step 2900: Training Cost = 1.67028, Validation Cost = 1.68800\n",
            "Step 3000: Training Cost = 1.65128, Validation Cost = 1.65581\n",
            "Step 3100: Training Cost = 1.57714, Validation Cost = 1.62475\n",
            "Step 3200: Training Cost = 1.57373, Validation Cost = 1.59187\n",
            "Step 3300: Training Cost = 1.52343, Validation Cost = 1.55991\n",
            "Step 3400: Training Cost = 1.49141, Validation Cost = 1.52715\n",
            "Step 3500: Training Cost = 1.53497, Validation Cost = 1.49480\n",
            "Step 3600: Training Cost = 1.47546, Validation Cost = 1.46425\n",
            "Step 3700: Training Cost = 1.50204, Validation Cost = 1.43513\n",
            "Step 3800: Training Cost = 1.35648, Validation Cost = 1.40797\n",
            "Step 3900: Training Cost = 1.38119, Validation Cost = 1.38522\n",
            "Step 4000: Training Cost = 1.32879, Validation Cost = 1.36229\n",
            "Step 4100: Training Cost = 1.31005, Validation Cost = 1.34279\n",
            "Step 4200: Training Cost = 1.28221, Validation Cost = 1.32542\n",
            "Step 4300: Training Cost = 1.37548, Validation Cost = 1.31326\n",
            "Step 4400: Training Cost = 1.26053, Validation Cost = 1.29741\n",
            "Step 4500: Training Cost = 1.24603, Validation Cost = 1.28415\n",
            "Step 4600: Training Cost = 1.22823, Validation Cost = 1.27206\n",
            "Step 4700: Training Cost = 1.34764, Validation Cost = 1.26181\n",
            "Step 4800: Training Cost = 1.21519, Validation Cost = 1.24823\n",
            "Step 4900: Training Cost = 1.18216, Validation Cost = 1.23600\n",
            "Step 5000: Training Cost = 1.11378, Validation Cost = 1.22506\n",
            "Step 5100: Training Cost = 1.20772, Validation Cost = 1.21592\n",
            "Step 5200: Training Cost = 1.21868, Validation Cost = 1.20354\n",
            "Step 5300: Training Cost = 1.27062, Validation Cost = 1.19289\n",
            "Step 5400: Training Cost = 1.19946, Validation Cost = 1.17973\n",
            "Step 5500: Training Cost = 1.12443, Validation Cost = 1.16728\n",
            "Step 5600: Training Cost = 1.14232, Validation Cost = 1.15507\n",
            "Step 5700: Training Cost = 1.34109, Validation Cost = 1.14260\n",
            "Step 5800: Training Cost = 1.09684, Validation Cost = 1.12944\n",
            "Step 5900: Training Cost = 1.04112, Validation Cost = 1.11656\n",
            "Step 6000: Training Cost = 1.00483, Validation Cost = 1.09692\n",
            "Step 6100: Training Cost = 0.93933, Validation Cost = 1.08264\n",
            "Step 6200: Training Cost = 1.07009, Validation Cost = 1.06230\n",
            "Step 6300: Training Cost = 1.03791, Validation Cost = 1.04626\n",
            "Step 6400: Training Cost = 1.03818, Validation Cost = 1.02607\n",
            "Step 6500: Training Cost = 1.03717, Validation Cost = 1.00569\n",
            "Step 6600: Training Cost = 0.95655, Validation Cost = 0.99295\n",
            "Step 6700: Training Cost = 1.02251, Validation Cost = 0.97155\n",
            "Step 6800: Training Cost = 0.93603, Validation Cost = 0.95682\n",
            "Step 6900: Training Cost = 1.00954, Validation Cost = 0.93902\n",
            "Step 7000: Training Cost = 0.86843, Validation Cost = 0.92862\n",
            "Step 7100: Training Cost = 0.95970, Validation Cost = 0.91072\n",
            "Step 7200: Training Cost = 0.85670, Validation Cost = 0.89756\n",
            "Step 7300: Training Cost = 0.79588, Validation Cost = 0.88569\n",
            "Step 7400: Training Cost = 0.94206, Validation Cost = 0.87220\n",
            "Step 7500: Training Cost = 0.86089, Validation Cost = 0.85912\n",
            "Step 7600: Training Cost = 0.85388, Validation Cost = 0.84786\n",
            "Step 7700: Training Cost = 0.87250, Validation Cost = 0.83230\n",
            "Step 7800: Training Cost = 0.77870, Validation Cost = 0.83055\n",
            "Step 7900: Training Cost = 0.72643, Validation Cost = 0.80894\n",
            "Step 8000: Training Cost = 0.75199, Validation Cost = 0.79373\n",
            "Step 8100: Training Cost = 0.74639, Validation Cost = 0.77917\n",
            "Step 8200: Training Cost = 0.74788, Validation Cost = 0.76801\n",
            "Step 8300: Training Cost = 0.70483, Validation Cost = 0.75143\n",
            "Step 8400: Training Cost = 0.73927, Validation Cost = 0.74303\n",
            "Step 8500: Training Cost = 0.66105, Validation Cost = 0.72811\n",
            "Step 8600: Training Cost = 0.71625, Validation Cost = 0.72192\n",
            "Step 8700: Training Cost = 0.63219, Validation Cost = 0.70755\n",
            "Step 8800: Training Cost = 0.63191, Validation Cost = 0.69893\n",
            "Step 8900: Training Cost = 0.61256, Validation Cost = 0.68855\n",
            "Step 9000: Training Cost = 0.76866, Validation Cost = 0.68745\n",
            "Step 9100: Training Cost = 0.57652, Validation Cost = 0.67434\n",
            "Step 9200: Training Cost = 0.57541, Validation Cost = 0.66504\n",
            "Step 9300: Training Cost = 0.64335, Validation Cost = 0.65766\n",
            "Step 9400: Training Cost = 0.66859, Validation Cost = 0.65319\n",
            "Step 9500: Training Cost = 0.58365, Validation Cost = 0.64428\n",
            "Step 9600: Training Cost = 0.59047, Validation Cost = 0.63634\n",
            "Step 9700: Training Cost = 0.47939, Validation Cost = 0.63023\n",
            "Step 9800: Training Cost = 0.59842, Validation Cost = 0.62997\n",
            "Step 9900: Training Cost = 0.55456, Validation Cost = 0.61801\n",
            "Step 10000: Training Cost = 0.69004, Validation Cost = 0.61255\n",
            "Step 10100: Training Cost = 0.60602, Validation Cost = 0.60883\n",
            "Step 10200: Training Cost = 0.53668, Validation Cost = 0.59913\n",
            "Step 10300: Training Cost = 0.65853, Validation Cost = 0.59372\n",
            "Step 10400: Training Cost = 0.80814, Validation Cost = 0.58741\n",
            "Step 10500: Training Cost = 0.54217, Validation Cost = 0.58232\n",
            "Step 10600: Training Cost = 0.50372, Validation Cost = 0.57926\n",
            "Step 10700: Training Cost = 0.42348, Validation Cost = 0.57108\n",
            "Step 10800: Training Cost = 0.45166, Validation Cost = 0.56820\n",
            "Step 10900: Training Cost = 0.55738, Validation Cost = 0.55883\n",
            "Step 11000: Training Cost = 0.51686, Validation Cost = 0.55759\n",
            "Step 11100: Training Cost = 0.45519, Validation Cost = 0.54938\n",
            "Step 11200: Training Cost = 0.56693, Validation Cost = 0.54335\n",
            "Step 11300: Training Cost = 0.46366, Validation Cost = 0.53909\n",
            "Step 11400: Training Cost = 0.51448, Validation Cost = 0.53804\n",
            "Step 11500: Training Cost = 0.49397, Validation Cost = 0.52954\n",
            "Step 11600: Training Cost = 0.63558, Validation Cost = 0.52554\n",
            "Step 11700: Training Cost = 0.49635, Validation Cost = 0.52434\n",
            "Step 11800: Training Cost = 0.57358, Validation Cost = 0.51814\n",
            "Step 11900: Training Cost = 0.51872, Validation Cost = 0.51240\n",
            "Step 12000: Training Cost = 0.45718, Validation Cost = 0.50779\n",
            "Step 12100: Training Cost = 0.50118, Validation Cost = 0.50497\n",
            "Step 12200: Training Cost = 0.49391, Validation Cost = 0.49821\n",
            "Step 12300: Training Cost = 0.48783, Validation Cost = 0.49484\n",
            "Step 12400: Training Cost = 0.55031, Validation Cost = 0.49002\n",
            "Step 12500: Training Cost = 0.43262, Validation Cost = 0.49431\n",
            "Step 12600: Training Cost = 0.39905, Validation Cost = 0.48610\n",
            "Step 12700: Training Cost = 0.43076, Validation Cost = 0.47976\n",
            "Step 12800: Training Cost = 0.45024, Validation Cost = 0.47577\n",
            "Step 12900: Training Cost = 0.46378, Validation Cost = 0.47545\n",
            "Step 13000: Training Cost = 0.39642, Validation Cost = 0.46766\n",
            "Step 13100: Training Cost = 0.41216, Validation Cost = 0.46765\n",
            "Early stopping activated.\n",
            "Final Training Cost: 0.4121646285469284\n",
            "Final Validation Cost: 0.4676450678077275\n",
            "Accuracy on validation: 0.876\n",
            "Epoch: 69, Iterations: 13100\n",
            "\n",
            "Average Magnitude of Weights ('without' L2):\n",
            "This run is without batch normalization. (use_bnorm=False)\n",
            "Layer 1: Average Magnitude = 0.01926\n",
            "Layer 2: Average Magnitude = 0.22563\n",
            "Layer 3: Average Magnitude = 0.43374\n",
            "Layer 4: Average Magnitude = 0.49053\n",
            "Accuarcy on train:  0.88355\n",
            "Accuarcy on test:  0.8822\n",
            "The time it takes: 48.68 seconds\n"
          ]
        }
      ],
      "source": [
        "past = time.time()\n",
        "params, loss_lst = l_layer_model(x_train.T, y_train.T, [x_train.shape[1], 20, 7, 5, 10],\n",
        "                                 learning_rate=0.009, num_iterations=100,\n",
        "                                 batch_size=256, use_bnorm=False)\n",
        "\n",
        "print(\"Accuarcy on train: \", predict(x_train.T, y_train.T, params))\n",
        "print(\"Accuarcy on test: \", predict(x_test.T, y_test.T, params,))\n",
        "\n",
        "print(f\"The time it takes: {np.round(time.time() - past, 2)} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sidcnbqLahJq",
        "outputId": "a23b88f3-4d36-41ac-dafe-25569ca08d78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100: Training Cost = 2.12300, Validation Cost = 2.12958\n",
            "Step 200: Training Cost = 2.00878, Validation Cost = 1.99921\n",
            "Step 300: Training Cost = 1.89166, Validation Cost = 1.91129\n",
            "Step 400: Training Cost = 1.81946, Validation Cost = 1.83523\n",
            "Step 500: Training Cost = 1.71411, Validation Cost = 1.76397\n",
            "Step 600: Training Cost = 1.69367, Validation Cost = 1.70335\n",
            "Step 700: Training Cost = 1.61979, Validation Cost = 1.63817\n",
            "Step 800: Training Cost = 1.56912, Validation Cost = 1.57706\n",
            "Step 900: Training Cost = 1.54291, Validation Cost = 1.51601\n",
            "Step 1000: Training Cost = 1.44681, Validation Cost = 1.46045\n",
            "Step 1100: Training Cost = 1.35723, Validation Cost = 1.40691\n",
            "Step 1200: Training Cost = 1.29295, Validation Cost = 1.35561\n",
            "Step 1300: Training Cost = 1.31666, Validation Cost = 1.30986\n",
            "Step 1400: Training Cost = 1.32658, Validation Cost = 1.27529\n",
            "Step 1500: Training Cost = 1.19009, Validation Cost = 1.23859\n",
            "Step 1600: Training Cost = 1.20094, Validation Cost = 1.20442\n",
            "Step 1700: Training Cost = 1.10688, Validation Cost = 1.17451\n",
            "Step 1800: Training Cost = 1.16466, Validation Cost = 1.14728\n",
            "Step 1900: Training Cost = 1.07790, Validation Cost = 1.12152\n",
            "Step 2000: Training Cost = 1.12199, Validation Cost = 1.09831\n",
            "Step 2100: Training Cost = 1.13876, Validation Cost = 1.07446\n",
            "Step 2200: Training Cost = 1.15248, Validation Cost = 1.05436\n",
            "Step 2300: Training Cost = 1.01639, Validation Cost = 1.03643\n",
            "Step 2400: Training Cost = 1.06840, Validation Cost = 1.01779\n",
            "Step 2500: Training Cost = 0.94531, Validation Cost = 0.99734\n",
            "Step 2600: Training Cost = 0.89979, Validation Cost = 0.97906\n",
            "Step 2700: Training Cost = 0.95675, Validation Cost = 0.95617\n",
            "Step 2800: Training Cost = 0.90593, Validation Cost = 0.94348\n",
            "Step 2900: Training Cost = 0.94775, Validation Cost = 0.92673\n",
            "Step 3000: Training Cost = 0.91179, Validation Cost = 0.91050\n",
            "Step 3100: Training Cost = 0.87983, Validation Cost = 0.89766\n",
            "Step 3200: Training Cost = 0.84383, Validation Cost = 0.88970\n",
            "Step 3300: Training Cost = 0.78981, Validation Cost = 0.87647\n",
            "Step 3400: Training Cost = 0.85407, Validation Cost = 0.85646\n",
            "Step 3500: Training Cost = 0.90099, Validation Cost = 0.84964\n",
            "Step 3600: Training Cost = 0.84324, Validation Cost = 0.84157\n",
            "Step 3700: Training Cost = 0.86505, Validation Cost = 0.82391\n",
            "Step 3800: Training Cost = 0.88011, Validation Cost = 0.81950\n",
            "Step 3900: Training Cost = 0.76607, Validation Cost = 0.80764\n",
            "Step 4000: Training Cost = 0.73821, Validation Cost = 0.78806\n",
            "Step 4100: Training Cost = 0.74574, Validation Cost = 0.77949\n",
            "Step 4200: Training Cost = 0.76115, Validation Cost = 0.77111\n",
            "Step 4300: Training Cost = 0.82759, Validation Cost = 0.75414\n",
            "Step 4400: Training Cost = 0.74662, Validation Cost = 0.74644\n",
            "Step 4500: Training Cost = 0.69916, Validation Cost = 0.73793\n",
            "Step 4600: Training Cost = 0.73231, Validation Cost = 0.72906\n",
            "Step 4700: Training Cost = 0.75948, Validation Cost = 0.71509\n",
            "Step 4800: Training Cost = 0.71163, Validation Cost = 0.70046\n",
            "Step 4900: Training Cost = 0.71604, Validation Cost = 0.69441\n",
            "Step 5000: Training Cost = 0.71726, Validation Cost = 0.67960\n",
            "Step 5100: Training Cost = 0.64517, Validation Cost = 0.67249\n",
            "Step 5200: Training Cost = 0.68516, Validation Cost = 0.66388\n",
            "Step 5300: Training Cost = 0.67442, Validation Cost = 0.65558\n",
            "Step 5400: Training Cost = 0.65726, Validation Cost = 0.64435\n",
            "Step 5500: Training Cost = 0.59984, Validation Cost = 0.63632\n",
            "Step 5600: Training Cost = 0.68006, Validation Cost = 0.63129\n",
            "Step 5700: Training Cost = 0.65890, Validation Cost = 0.61816\n",
            "Step 5800: Training Cost = 0.59503, Validation Cost = 0.61352\n",
            "Step 5900: Training Cost = 0.58417, Validation Cost = 0.61439\n",
            "Step 6000: Training Cost = 0.57956, Validation Cost = 0.60490\n",
            "Step 6100: Training Cost = 0.62644, Validation Cost = 0.60980\n",
            "Step 6200: Training Cost = 0.59594, Validation Cost = 0.58949\n",
            "Step 6300: Training Cost = 0.52958, Validation Cost = 0.57931\n",
            "Step 6400: Training Cost = 0.47101, Validation Cost = 0.56947\n",
            "Step 6500: Training Cost = 0.54630, Validation Cost = 0.56713\n",
            "Step 6600: Training Cost = 0.49400, Validation Cost = 0.55649\n",
            "Step 6700: Training Cost = 0.54585, Validation Cost = 0.55431\n",
            "Step 6800: Training Cost = 0.61386, Validation Cost = 0.54973\n",
            "Step 6900: Training Cost = 0.56580, Validation Cost = 0.53621\n",
            "Step 7000: Training Cost = 0.49169, Validation Cost = 0.54284\n",
            "Step 7100: Training Cost = 0.49211, Validation Cost = 0.52882\n",
            "Step 7200: Training Cost = 0.45275, Validation Cost = 0.51966\n",
            "Step 7300: Training Cost = 0.48133, Validation Cost = 0.51503\n",
            "Step 7400: Training Cost = 0.45689, Validation Cost = 0.51483\n",
            "Step 7500: Training Cost = 0.47803, Validation Cost = 0.50377\n",
            "Step 7600: Training Cost = 0.48463, Validation Cost = 0.50853\n",
            "Step 7700: Training Cost = 0.51180, Validation Cost = 0.49570\n",
            "Step 7800: Training Cost = 0.49803, Validation Cost = 0.49532\n",
            "Step 7900: Training Cost = 0.46444, Validation Cost = 0.48857\n",
            "Step 8000: Training Cost = 0.40387, Validation Cost = 0.48744\n",
            "Step 8100: Training Cost = 0.43783, Validation Cost = 0.48220\n",
            "Step 8200: Training Cost = 0.50250, Validation Cost = 0.48218\n",
            "Early stopping activated.\n",
            "Final Training Cost: 0.5025023785429055\n",
            "Final Validation Cost: 0.4821833730656054\n",
            "Accuracy on validation: 0.88925\n",
            "Epoch: 43, Iterations: 8200\n",
            "\n",
            "Average Magnitude of Weights ('without' L2):\n",
            "This run is with batch normalization. (use_bnorm=True)\n",
            "Layer 1: Average Magnitude = 0.27992\n",
            "Layer 2: Average Magnitude = 1.17542\n",
            "Layer 3: Average Magnitude = 1.33284\n",
            "Layer 4: Average Magnitude = 0.87857\n",
            "Accuarcy on train:  0.8977166666666667\n",
            "Accuarcy on test:  0.896\n",
            "The time it takes: 36.95 seconds\n"
          ]
        }
      ],
      "source": [
        "past = time.time()\n",
        "params, loss_lst = l_layer_model(x_train.T, y_train.T, [x_train.shape[1], 20, 7, 5, 10],\n",
        "                                 learning_rate=0.009, num_iterations=100,\n",
        "                                 batch_size=256, use_bnorm=True)\n",
        "\n",
        "print(\"Accuarcy on train: \", predict(x_train.T, y_train.T, params, True))\n",
        "print(\"Accuarcy on test: \", predict(x_test.T, y_test.T, params, True))\n",
        "\n",
        "print(f\"The time it takes: {np.round(time.time() - past, 2)} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L2"
      ],
      "metadata": {
        "id": "m8EiBDKv6LHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(AL, Y, params, lambd=0):\n",
        "    \"\"\"\n",
        "    Compute the categorical cross-entropy cost with L2 regularization.\n",
        "\n",
        "    Arguments:\n",
        "    ----------\n",
        "    AL : numpy.ndarray\n",
        "        Probability vector from softmax, shape (num_classes, number of examples).\n",
        "    Y : numpy.ndarray\n",
        "        Ground truth labels, one-hot encoded, shape (num_classes, number of examples).\n",
        "    params : dict\n",
        "        Dictionary containing the parameters of the network (weights and biases).\n",
        "    lambd : float\n",
        "        Regularization strength (default is 0, meaning no regularization).\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    cost : float\n",
        "        The total cost (cross-entropy + L2 regularization penalty).\n",
        "    \"\"\"\n",
        "    m = Y.shape[1]  # Number of examples\n",
        "\n",
        "    # Cross-entropy cost\n",
        "    cross_entropy_cost = -np.sum(Y * np.log(AL + 1e-8)) / (m + 1e-8)  # Added epsilon for numerical stability\n",
        "\n",
        "    # L2 regularization cost\n",
        "    l2_penalty = 0\n",
        "    for key in params:\n",
        "        if key.startswith(\"W\"):  # Apply L2 regularization only to weights\n",
        "            l2_penalty += np.sum(np.square(params[key]))\n",
        "    l2_penalty = (lambd / (2 * m)) * l2_penalty\n",
        "\n",
        "    # Total cost\n",
        "    cost = cross_entropy_cost + l2_penalty\n",
        "\n",
        "    return cost\n"
      ],
      "metadata": {
        "id": "uALh_Q_N38-O"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def l_model_backward(AL, Y, caches, parameters, lambd=0):\n",
        "    \"\"\"\n",
        "    Implements the backward propagation for the entire network with L2 regularization.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    AL : numpy.ndarray\n",
        "        Post-activation output of the final layer.\n",
        "    Y : numpy.ndarray\n",
        "        True labels in one-hot encoded format.\n",
        "    caches : list\n",
        "        List of caches for each layer, generated during forward propagation.\n",
        "    parameters : dict\n",
        "        Dictionary containing the DNN parameters (weights and biases).\n",
        "    lambd : float\n",
        "        Regularization strength (default is 0, meaning no regularization).\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    grads : dict\n",
        "        Dictionary containing gradients for all layers.\n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches)  # Number of layers in the model\n",
        "    m = AL.shape[1]  # Number of examples\n",
        "\n",
        "    # Backpropagation for the final layer (Softmax)\n",
        "    final_cache = caches[-1]\n",
        "    softmax_cache = {\n",
        "        \"linear_cache\": final_cache[\"linear_cache\"],\n",
        "        \"activation_cache\": {\"Z\": final_cache[\"activation_cache\"], \"Y\": Y},\n",
        "    }\n",
        "    dA_prev, dW, db = linear_activation_backward(AL, softmax_cache, activation=\"softmax\")\n",
        "    if lambd > 0:\n",
        "        dW += (lambd / m) * parameters[f\"W{L}\"]  # Add L2 regularization term to dW\n",
        "    grads[f\"dA{L}\"] = dA_prev\n",
        "    grads[f\"dW{L}\"] = dW\n",
        "    grads[f\"db{L}\"] = db\n",
        "\n",
        "    # Backpropagation for hidden layers (ReLU)\n",
        "    for l in reversed(range(1, L)):  # From L-1 to 1\n",
        "        current_cache = caches[l - 1]\n",
        "        dA_prev, dW, db = linear_activation_backward(dA_prev, current_cache, activation=\"relu\")\n",
        "        if lambd > 0:\n",
        "            dW += (lambd / m) * parameters[f\"W{l}\"]  # Add L2 regularization term to dW\n",
        "        grads[f\"dA{l}\"] = dA_prev\n",
        "        grads[f\"dW{l}\"] = dW\n",
        "        grads[f\"db{l}\"] = db\n",
        "\n",
        "    return grads\n"
      ],
      "metadata": {
        "id": "MvzBc_ml6y4i"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def l_layer_model(X, Y, layers_dims, learning_rate, num_iterations=100, batch_size=256, use_bnorm=False,lamda=0.001):\n",
        "    \"\"\"\n",
        "    Trains a deep neural network with optional batch normalization.\n",
        "\n",
        "    Parameters:\n",
        "    ---------\n",
        "    X : np.ndarray\n",
        "        Input data with shape (features, number_of_samples).\n",
        "    Y : np.ndarray\n",
        "        True labels (one-hot encoded) with shape (classes, number_of_samples).\n",
        "    layers_dims : list\n",
        "        List specifying the size of each layer in the network.\n",
        "    learning_rate : float\n",
        "        Learning rate for the gradient descent updates.\n",
        "    num_iterations : int, optional\n",
        "        Total number of epochs for training. Default is 100.\n",
        "    batch_size : int, optional\n",
        "        Number of samples per mini-batch. Default is 32.\n",
        "    use_bnorm : bool, optional\n",
        "        If True, apply batch normalization during training.\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    parameters : dict\n",
        "        The learned weights and biases of the network.\n",
        "    costs_train : list\n",
        "        Cost values recorded at checkpoints during training.\n",
        "    \"\"\"\n",
        "    # Lists to track costs for training and validation\n",
        "    costs_train = []\n",
        "    costs_valid = []\n",
        "\n",
        "    # Split data into training (80%) and validation (20%)\n",
        "    num_samples = X.shape[1]\n",
        "    train_size = int(0.8 * num_samples)\n",
        "    indices = np.random.permutation(num_samples)\n",
        "    train_indices = indices[:train_size]\n",
        "    valid_indices = indices[train_size:]\n",
        "\n",
        "    X_train, X_valid = X[:, train_indices], X[:, valid_indices]\n",
        "    Y_train, Y_valid = Y[:, train_indices], Y[:, valid_indices]\n",
        "\n",
        "    # Initialize parameters for the network\n",
        "    parameters = initialize_parameters(layers_dims)\n",
        "\n",
        "    # Counters for iterations and epochs\n",
        "    batch_counter = 1\n",
        "    total_epochs = 0\n",
        "\n",
        "    # Training loop over epochs\n",
        "    for epoch in range(num_iterations):\n",
        "        # Mini-batch loop\n",
        "        for start in range(0, X_train.shape[1], batch_size):\n",
        "            end = min(start + batch_size, X_train.shape[1])\n",
        "            X_batch = X_train[:, start:end]\n",
        "            Y_batch = Y_train[:, start:end]\n",
        "\n",
        "            # Forward pass\n",
        "            AL, caches = l_model_forward(X_batch, parameters, use_bnorm)\n",
        "\n",
        "            # Backward pass\n",
        "            gradients = l_model_backward(AL, Y_batch, caches,lamda)\n",
        "\n",
        "            # Update parameters\n",
        "            parameters = update_parameters(parameters, gradients, learning_rate)\n",
        "\n",
        "            # Track the epoch number\n",
        "            total_epochs = epoch\n",
        "\n",
        "            # Periodic cost logging\n",
        "            if batch_counter % 100 == 0:\n",
        "                # Compute training cost\n",
        "                train_cost = compute_cost(AL, Y_batch,parameters,lamda)\n",
        "                costs_train.append(train_cost)\n",
        "\n",
        "                # Compute validation cost\n",
        "                AL_valid, _ = l_model_forward(X_valid, parameters, use_bnorm)\n",
        "                valid_cost = compute_cost(AL_valid, Y_valid,parameters,lamda)\n",
        "                costs_valid.append(valid_cost)\n",
        "\n",
        "                print(f\"Step {batch_counter}: Training Cost = {train_cost:.5f}, Validation Cost = {valid_cost:.5f}\")\n",
        "\n",
        "                # Early stopping: Check validation cost improvement\n",
        "                if len(costs_valid) > 1 and abs(costs_valid[-2] - costs_valid[-1]) < 0.0001:\n",
        "                    validation_accuracy = predict(X_valid, Y_valid, parameters, use_bnorm)\n",
        "                    print(\"Early stopping activated.\")\n",
        "                    print(f\"Final Training Cost: {train_cost}\")\n",
        "                    print(f\"Final Validation Cost: {valid_cost}\")\n",
        "                    print(f\"Accuracy on validation: {validation_accuracy}\")\n",
        "                    print(f\"Epoch: {total_epochs}, Iterations: {batch_counter}\")\n",
        "\n",
        "\n",
        "                    # Compute average magnitudes of weights for each layer\n",
        "                    print(\"\\nAverage Magnitude of Weights (with L2):\")\n",
        "                    print(f\"This run is {'with' if use_bnorm else 'without'} batch normalization. (use_bnorm={use_bnorm})\")\n",
        "                    for l in range(1, len(layers_dims)):\n",
        "                        W = parameters[f\"W{l}\"]\n",
        "                        avg_magnitude = np.mean(np.abs(W))\n",
        "                        print(f\"Layer {l}: Average Magnitude = {avg_magnitude:.5f}\")\n",
        "\n",
        "                    return parameters, costs_train\n",
        "\n",
        "            batch_counter += 1\n",
        "\n",
        "    # Final cost computations\n",
        "    final_train_cost = compute_cost(AL, Y_batch,parameters,lamda)\n",
        "    final_valid_cost = compute_cost(AL_valid, Y_valid,parameters,lamda)\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "    print(f\"Final Training Cost: {final_train_cost}\")\n",
        "    print(f\"Final Validation Cost: {final_valid_cost}\")\n",
        "    print(\"Accuracy on validation: \", predict(X_valid, Y_valid, parameters, use_bnorm))\n",
        "    print(f\"Epoch: {total_epochs}, Iterations: {batch_counter}\")\n",
        "\n",
        "\n",
        "    # Compute average magnitudes of weights for each layer\n",
        "    print(\"\\nAverage Magnitude of Weights (without L2):\")\n",
        "    print(f\"This run is {'with' if use_bnorm else 'without'} batch normalization. (use_bnorm={use_bnorm})\")\n",
        "    for l in range(1, len(layers_dims)):\n",
        "        W = parameters[f\"W{l}\"]\n",
        "        avg_magnitude = np.mean(np.abs(W))\n",
        "        print(f\"Layer {l}: Average Magnitude = {avg_magnitude:.5f}\")\n",
        "\n",
        "    return parameters, costs_train\n"
      ],
      "metadata": {
        "id": "i7tr2NRR-Xls"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values to the range [0, 1]\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Flatten the images (convert 2D arrays into 1D arrays)\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)  # Flatten each image\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)    # Flatten each image\n",
        "\n",
        "# Convert labels to one-hot encoded format\n",
        "num_classes = 10\n",
        "y_train_one_hot = np.eye(num_classes)[y_train]  # One-hot encoding for training labels\n",
        "y_test_one_hot = np.eye(num_classes)[y_test]    # One-hot encoding for test labels\n",
        "\n",
        "# Assign the one-hot encoded labels back to the original variables\n",
        "y_train = y_train_one_hot\n",
        "y_test = y_test_one_hot\n"
      ],
      "metadata": {
        "id": "dMrqn9bs4SDQ"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "past = time.time()\n",
        "params, loss_lst = l_layer_model(x_train.T, y_train.T, [x_train.shape[1], 20, 7, 5, 10],\n",
        "                                 learning_rate=0.009, num_iterations=100,\n",
        "                                 batch_size=256, use_bnorm=False,lamda=0.001)\n",
        "\n",
        "print(\"Accuracy on train: \", predict(x_train.T, y_train.T, params))\n",
        "print(\"Accuracy on test: \", predict(x_test.T, y_test.T, params))\n",
        "\n",
        "print(f\"The time it takes: {np.round(time.time() - past, 2)} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTU344sZwON1",
        "outputId": "23e20c77-3f6d-473f-ec12-2b0559d43b01"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100: Training Cost = 2.30219, Validation Cost = 2.30218\n",
            "Step 200: Training Cost = 2.30168, Validation Cost = 2.30168\n",
            "Step 300: Training Cost = 2.30150, Validation Cost = 2.30106\n",
            "Step 400: Training Cost = 2.30034, Validation Cost = 2.30052\n",
            "Step 500: Training Cost = 2.30066, Validation Cost = 2.29985\n",
            "Step 600: Training Cost = 2.29799, Validation Cost = 2.29923\n",
            "Step 700: Training Cost = 2.29959, Validation Cost = 2.29855\n",
            "Step 800: Training Cost = 2.29883, Validation Cost = 2.29784\n",
            "Step 900: Training Cost = 2.29708, Validation Cost = 2.29707\n",
            "Step 1000: Training Cost = 2.29583, Validation Cost = 2.29619\n",
            "Step 1100: Training Cost = 2.29392, Validation Cost = 2.29523\n",
            "Step 1200: Training Cost = 2.29880, Validation Cost = 2.29413\n",
            "Step 1300: Training Cost = 2.28662, Validation Cost = 2.29290\n",
            "Step 1400: Training Cost = 2.29350, Validation Cost = 2.29145\n",
            "Step 1500: Training Cost = 2.28749, Validation Cost = 2.28980\n",
            "Step 1600: Training Cost = 2.28899, Validation Cost = 2.28783\n",
            "Step 1700: Training Cost = 2.29022, Validation Cost = 2.28558\n",
            "Step 1800: Training Cost = 2.28372, Validation Cost = 2.28283\n",
            "Step 1900: Training Cost = 2.28281, Validation Cost = 2.27965\n",
            "Step 2000: Training Cost = 2.28170, Validation Cost = 2.27570\n",
            "Step 2100: Training Cost = 2.27622, Validation Cost = 2.27103\n",
            "Step 2200: Training Cost = 2.27053, Validation Cost = 2.26516\n",
            "Step 2300: Training Cost = 2.25768, Validation Cost = 2.25792\n",
            "Step 2400: Training Cost = 2.25621, Validation Cost = 2.24874\n",
            "Step 2500: Training Cost = 2.24798, Validation Cost = 2.23687\n",
            "Step 2600: Training Cost = 2.22784, Validation Cost = 2.22131\n",
            "Step 2700: Training Cost = 2.20893, Validation Cost = 2.20064\n",
            "Step 2800: Training Cost = 2.17943, Validation Cost = 2.17391\n",
            "Step 2900: Training Cost = 2.14572, Validation Cost = 2.14164\n",
            "Step 3000: Training Cost = 2.11310, Validation Cost = 2.10660\n",
            "Step 3100: Training Cost = 2.08191, Validation Cost = 2.07226\n",
            "Step 3200: Training Cost = 2.06901, Validation Cost = 2.04068\n",
            "Step 3300: Training Cost = 2.04161, Validation Cost = 2.01229\n",
            "Step 3400: Training Cost = 1.96696, Validation Cost = 1.98660\n",
            "Step 3500: Training Cost = 1.94753, Validation Cost = 1.96255\n",
            "Step 3600: Training Cost = 1.94490, Validation Cost = 1.93919\n",
            "Step 3700: Training Cost = 1.89084, Validation Cost = 1.91522\n",
            "Step 3800: Training Cost = 1.88622, Validation Cost = 1.88966\n",
            "Step 3900: Training Cost = 1.89454, Validation Cost = 1.85975\n",
            "Step 4000: Training Cost = 1.78161, Validation Cost = 1.82417\n",
            "Step 4100: Training Cost = 1.77177, Validation Cost = 1.78191\n",
            "Step 4200: Training Cost = 1.77885, Validation Cost = 1.73597\n",
            "Step 4300: Training Cost = 1.68460, Validation Cost = 1.69254\n",
            "Step 4400: Training Cost = 1.64167, Validation Cost = 1.65314\n",
            "Step 4500: Training Cost = 1.64051, Validation Cost = 1.61741\n",
            "Step 4600: Training Cost = 1.65513, Validation Cost = 1.58213\n",
            "Step 4700: Training Cost = 1.50160, Validation Cost = 1.54782\n",
            "Step 4800: Training Cost = 1.52192, Validation Cost = 1.51420\n",
            "Step 4900: Training Cost = 1.45545, Validation Cost = 1.48300\n",
            "Step 5000: Training Cost = 1.48936, Validation Cost = 1.45525\n",
            "Step 5100: Training Cost = 1.40398, Validation Cost = 1.42979\n",
            "Step 5200: Training Cost = 1.44241, Validation Cost = 1.40804\n",
            "Step 5300: Training Cost = 1.35096, Validation Cost = 1.38893\n",
            "Step 5400: Training Cost = 1.38811, Validation Cost = 1.37235\n",
            "Step 5500: Training Cost = 1.36301, Validation Cost = 1.35608\n",
            "Step 5600: Training Cost = 1.33861, Validation Cost = 1.34188\n",
            "Step 5700: Training Cost = 1.26320, Validation Cost = 1.32856\n",
            "Step 5800: Training Cost = 1.23292, Validation Cost = 1.31670\n",
            "Step 5900: Training Cost = 1.39134, Validation Cost = 1.30624\n",
            "Step 6000: Training Cost = 1.24395, Validation Cost = 1.29399\n",
            "Step 6100: Training Cost = 1.36046, Validation Cost = 1.28268\n",
            "Step 6200: Training Cost = 1.25149, Validation Cost = 1.27215\n",
            "Step 6300: Training Cost = 1.23700, Validation Cost = 1.26157\n",
            "Step 6400: Training Cost = 1.37024, Validation Cost = 1.25158\n",
            "Step 6500: Training Cost = 1.23722, Validation Cost = 1.24138\n",
            "Step 6600: Training Cost = 1.23815, Validation Cost = 1.23274\n",
            "Step 6700: Training Cost = 1.35675, Validation Cost = 1.22317\n",
            "Step 6800: Training Cost = 1.21257, Validation Cost = 1.21415\n",
            "Step 6900: Training Cost = 1.31994, Validation Cost = 1.20484\n",
            "Step 7000: Training Cost = 1.17253, Validation Cost = 1.19806\n",
            "Step 7100: Training Cost = 1.24186, Validation Cost = 1.18545\n",
            "Step 7200: Training Cost = 1.21563, Validation Cost = 1.17599\n",
            "Step 7300: Training Cost = 1.17668, Validation Cost = 1.16519\n",
            "Step 7400: Training Cost = 1.16394, Validation Cost = 1.15604\n",
            "Step 7500: Training Cost = 1.27765, Validation Cost = 1.14548\n",
            "Step 7600: Training Cost = 1.13436, Validation Cost = 1.13617\n",
            "Step 7700: Training Cost = 1.14267, Validation Cost = 1.12782\n",
            "Step 7800: Training Cost = 1.14786, Validation Cost = 1.11880\n",
            "Step 7900: Training Cost = 1.15056, Validation Cost = 1.10912\n",
            "Step 8000: Training Cost = 1.22516, Validation Cost = 1.10201\n",
            "Step 8100: Training Cost = 1.06380, Validation Cost = 1.09286\n",
            "Step 8200: Training Cost = 1.11487, Validation Cost = 1.08389\n",
            "Step 8300: Training Cost = 1.08526, Validation Cost = 1.07617\n",
            "Step 8400: Training Cost = 1.02057, Validation Cost = 1.06890\n",
            "Step 8500: Training Cost = 1.01078, Validation Cost = 1.06452\n",
            "Step 8600: Training Cost = 1.09922, Validation Cost = 1.05575\n",
            "Step 8700: Training Cost = 1.02813, Validation Cost = 1.04883\n",
            "Step 8800: Training Cost = 1.05598, Validation Cost = 1.04100\n",
            "Step 8900: Training Cost = 1.03734, Validation Cost = 1.03518\n",
            "Step 9000: Training Cost = 1.04125, Validation Cost = 1.02711\n",
            "Step 9100: Training Cost = 0.93535, Validation Cost = 1.02103\n",
            "Step 9200: Training Cost = 0.99423, Validation Cost = 1.01519\n",
            "Step 9300: Training Cost = 1.09557, Validation Cost = 1.00915\n",
            "Step 9400: Training Cost = 0.97371, Validation Cost = 0.99969\n",
            "Step 9500: Training Cost = 1.07517, Validation Cost = 0.99247\n",
            "Step 9600: Training Cost = 0.89760, Validation Cost = 0.98545\n",
            "Step 9700: Training Cost = 0.97437, Validation Cost = 0.98002\n",
            "Step 9800: Training Cost = 0.93575, Validation Cost = 0.97285\n",
            "Step 9900: Training Cost = 0.95371, Validation Cost = 0.96699\n",
            "Step 10000: Training Cost = 0.92654, Validation Cost = 0.95885\n",
            "Step 10100: Training Cost = 0.97377, Validation Cost = 0.95259\n",
            "Step 10200: Training Cost = 0.98247, Validation Cost = 0.94579\n",
            "Step 10300: Training Cost = 0.94290, Validation Cost = 0.94188\n",
            "Step 10400: Training Cost = 0.87846, Validation Cost = 0.92812\n",
            "Step 10500: Training Cost = 0.86051, Validation Cost = 0.92118\n",
            "Step 10600: Training Cost = 0.97187, Validation Cost = 0.91519\n",
            "Step 10700: Training Cost = 0.88874, Validation Cost = 0.90976\n",
            "Step 10800: Training Cost = 0.99238, Validation Cost = 0.89822\n",
            "Step 10900: Training Cost = 0.86639, Validation Cost = 0.89277\n",
            "Step 11000: Training Cost = 0.86111, Validation Cost = 0.88523\n",
            "Step 11100: Training Cost = 0.98327, Validation Cost = 0.87990\n",
            "Step 11200: Training Cost = 0.81558, Validation Cost = 0.87148\n",
            "Step 11300: Training Cost = 0.87350, Validation Cost = 0.86709\n",
            "Step 11400: Training Cost = 0.98299, Validation Cost = 0.86014\n",
            "Step 11500: Training Cost = 0.80850, Validation Cost = 0.85300\n",
            "Step 11600: Training Cost = 0.94148, Validation Cost = 0.85408\n",
            "Step 11700: Training Cost = 0.88899, Validation Cost = 0.84519\n",
            "Step 11800: Training Cost = 0.86628, Validation Cost = 0.83546\n",
            "Step 11900: Training Cost = 0.81978, Validation Cost = 0.83350\n",
            "Step 12000: Training Cost = 0.80274, Validation Cost = 0.82325\n",
            "Step 12100: Training Cost = 0.74762, Validation Cost = 0.81928\n",
            "Step 12200: Training Cost = 0.87972, Validation Cost = 0.81599\n",
            "Step 12300: Training Cost = 0.78869, Validation Cost = 0.81139\n",
            "Step 12400: Training Cost = 0.80697, Validation Cost = 0.80503\n",
            "Step 12500: Training Cost = 0.85739, Validation Cost = 0.79820\n",
            "Step 12600: Training Cost = 0.82967, Validation Cost = 0.79388\n",
            "Step 12700: Training Cost = 0.90809, Validation Cost = 0.79181\n",
            "Step 12800: Training Cost = 0.74718, Validation Cost = 0.78235\n",
            "Step 12900: Training Cost = 0.78603, Validation Cost = 0.77874\n",
            "Step 13000: Training Cost = 0.81506, Validation Cost = 0.77416\n",
            "Step 13100: Training Cost = 0.69958, Validation Cost = 0.76821\n",
            "Step 13200: Training Cost = 0.75397, Validation Cost = 0.77015\n",
            "Step 13300: Training Cost = 0.81015, Validation Cost = 0.76014\n",
            "Step 13400: Training Cost = 0.73243, Validation Cost = 0.75579\n",
            "Step 13500: Training Cost = 0.78739, Validation Cost = 0.75472\n",
            "Step 13600: Training Cost = 0.75980, Validation Cost = 0.75529\n",
            "Step 13700: Training Cost = 0.74510, Validation Cost = 0.74350\n",
            "Step 13800: Training Cost = 0.73416, Validation Cost = 0.74039\n",
            "Step 13900: Training Cost = 0.73353, Validation Cost = 0.73600\n",
            "Step 14000: Training Cost = 0.79827, Validation Cost = 0.73887\n",
            "Step 14100: Training Cost = 0.72166, Validation Cost = 0.72982\n",
            "Step 14200: Training Cost = 0.80749, Validation Cost = 0.72857\n",
            "Step 14300: Training Cost = 0.63545, Validation Cost = 0.72158\n",
            "Step 14400: Training Cost = 0.68184, Validation Cost = 0.71817\n",
            "Step 14500: Training Cost = 0.66910, Validation Cost = 0.71793\n",
            "Step 14600: Training Cost = 0.69658, Validation Cost = 0.74176\n",
            "Step 14700: Training Cost = 0.71804, Validation Cost = 0.70735\n",
            "Step 14800: Training Cost = 0.68012, Validation Cost = 0.70445\n",
            "Step 14900: Training Cost = 0.72618, Validation Cost = 0.70189\n",
            "Step 15000: Training Cost = 0.67950, Validation Cost = 0.70382\n",
            "Step 15100: Training Cost = 0.66555, Validation Cost = 0.69595\n",
            "Step 15200: Training Cost = 0.66480, Validation Cost = 0.69262\n",
            "Step 15300: Training Cost = 0.68111, Validation Cost = 0.70374\n",
            "Step 15400: Training Cost = 0.68087, Validation Cost = 0.69080\n",
            "Step 15500: Training Cost = 0.74934, Validation Cost = 0.68373\n",
            "Step 15600: Training Cost = 0.62991, Validation Cost = 0.68945\n",
            "Step 15700: Training Cost = 0.64588, Validation Cost = 0.68081\n",
            "Step 15800: Training Cost = 0.73061, Validation Cost = 0.67970\n",
            "Step 15900: Training Cost = 0.58206, Validation Cost = 0.67461\n",
            "Step 16000: Training Cost = 0.74860, Validation Cost = 0.67665\n",
            "Step 16100: Training Cost = 0.80744, Validation Cost = 0.66991\n",
            "Step 16200: Training Cost = 0.59059, Validation Cost = 0.66735\n",
            "Step 16300: Training Cost = 0.71355, Validation Cost = 0.66943\n",
            "Step 16400: Training Cost = 0.69324, Validation Cost = 0.67430\n",
            "Step 16500: Training Cost = 0.65627, Validation Cost = 0.66532\n",
            "Step 16600: Training Cost = 0.59975, Validation Cost = 0.65706\n",
            "Step 16700: Training Cost = 0.58110, Validation Cost = 0.65562\n",
            "Step 16800: Training Cost = 0.59102, Validation Cost = 0.66836\n",
            "Step 16900: Training Cost = 0.69811, Validation Cost = 0.68359\n",
            "Step 17000: Training Cost = 0.61651, Validation Cost = 0.65440\n",
            "Step 17100: Training Cost = 0.67538, Validation Cost = 0.67167\n",
            "Step 17200: Training Cost = 0.69553, Validation Cost = 0.65330\n",
            "Step 17300: Training Cost = 0.66386, Validation Cost = 0.66008\n",
            "Step 17400: Training Cost = 0.73020, Validation Cost = 0.66137\n",
            "Step 17500: Training Cost = 0.59048, Validation Cost = 0.63898\n",
            "Step 17600: Training Cost = 0.66456, Validation Cost = 0.63743\n",
            "Step 17700: Training Cost = 0.64269, Validation Cost = 0.63471\n",
            "Step 17800: Training Cost = 0.56119, Validation Cost = 0.64191\n",
            "Step 17900: Training Cost = 0.62870, Validation Cost = 0.63581\n",
            "Step 18000: Training Cost = 0.70919, Validation Cost = 0.65205\n",
            "Step 18100: Training Cost = 0.57107, Validation Cost = 0.63473\n",
            "Step 18200: Training Cost = 0.64630, Validation Cost = 0.62783\n",
            "Step 18300: Training Cost = 0.65895, Validation Cost = 0.66948\n",
            "Step 18400: Training Cost = 0.62771, Validation Cost = 0.62307\n",
            "Step 18500: Training Cost = 0.61732, Validation Cost = 0.62803\n",
            "Step 18600: Training Cost = 0.60607, Validation Cost = 0.61801\n",
            "Step 18700: Training Cost = 0.64010, Validation Cost = 0.63049\n",
            "Step 18800: Training Cost = 0.59466, Validation Cost = 0.61703\n",
            "Training complete.\n",
            "Final Training Cost: 0.5946603746012806\n",
            "Final Validation Cost: 0.6170265370224594\n",
            "Accuracy on validation:  0.8493333333333334\n",
            "Epoch: 99, Iterations: 18801\n",
            "\n",
            "Average Magnitude of Weights (without L2):\n",
            "This run is without batch normalization. (use_bnorm=False)\n",
            "Layer 1: Average Magnitude = 0.01991\n",
            "Layer 2: Average Magnitude = 0.24582\n",
            "Layer 3: Average Magnitude = 0.41696\n",
            "Layer 4: Average Magnitude = 0.42779\n",
            "Accuracy on train:  0.85385\n",
            "Accuracy on test:  0.8522\n",
            "The time it takes: 72.0 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "past = time.time()\n",
        "params, loss_lst = l_layer_model(x_train.T, y_train.T, [x_train.shape[1], 20, 7, 5, 10],\n",
        "                                 learning_rate=0.009, num_iterations=100,\n",
        "                                 batch_size=256, use_bnorm=True,lamda=0.001)\n",
        "\n",
        "print(\"Accuarcy on train: \", predict(x_train.T, y_train.T, params, True))\n",
        "print(\"Accuarcy on test: \", predict(x_test.T, y_test.T, params, True))\n",
        "\n",
        "print(f\"The time it takes: {np.round(time.time() - past, 2)} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAc7T7BIwqKd",
        "outputId": "a7079117-64a6-4aff-cc33-1860ebdc3cbd"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100: Training Cost = 2.16448, Validation Cost = 2.16424\n",
            "Step 200: Training Cost = 2.04966, Validation Cost = 2.04966\n",
            "Step 300: Training Cost = 2.00821, Validation Cost = 1.96802\n",
            "Step 400: Training Cost = 1.87288, Validation Cost = 1.88100\n",
            "Step 500: Training Cost = 1.81977, Validation Cost = 1.80244\n",
            "Step 600: Training Cost = 1.75071, Validation Cost = 1.72280\n",
            "Step 700: Training Cost = 1.63294, Validation Cost = 1.64756\n",
            "Step 800: Training Cost = 1.58684, Validation Cost = 1.57784\n",
            "Step 900: Training Cost = 1.49652, Validation Cost = 1.51583\n",
            "Step 1000: Training Cost = 1.45530, Validation Cost = 1.45923\n",
            "Step 1100: Training Cost = 1.43551, Validation Cost = 1.41013\n",
            "Step 1200: Training Cost = 1.45518, Validation Cost = 1.36640\n",
            "Step 1300: Training Cost = 1.27884, Validation Cost = 1.32881\n",
            "Step 1400: Training Cost = 1.33898, Validation Cost = 1.28988\n",
            "Step 1500: Training Cost = 1.28501, Validation Cost = 1.25679\n",
            "Step 1600: Training Cost = 1.23635, Validation Cost = 1.22542\n",
            "Step 1700: Training Cost = 1.15357, Validation Cost = 1.19790\n",
            "Step 1800: Training Cost = 1.23363, Validation Cost = 1.17124\n",
            "Step 1900: Training Cost = 1.15359, Validation Cost = 1.14693\n",
            "Step 2000: Training Cost = 1.12708, Validation Cost = 1.12434\n",
            "Step 2100: Training Cost = 1.04899, Validation Cost = 1.10339\n",
            "Step 2200: Training Cost = 1.11778, Validation Cost = 1.08059\n",
            "Step 2300: Training Cost = 1.04993, Validation Cost = 1.06102\n",
            "Step 2400: Training Cost = 1.13129, Validation Cost = 1.04105\n",
            "Step 2500: Training Cost = 1.04496, Validation Cost = 1.02069\n",
            "Step 2600: Training Cost = 0.98107, Validation Cost = 1.00677\n",
            "Step 2700: Training Cost = 1.01827, Validation Cost = 0.98576\n",
            "Step 2800: Training Cost = 0.91654, Validation Cost = 0.97339\n",
            "Step 2900: Training Cost = 0.95678, Validation Cost = 0.95228\n",
            "Step 3000: Training Cost = 1.00663, Validation Cost = 0.93609\n",
            "Step 3100: Training Cost = 0.91633, Validation Cost = 0.92098\n",
            "Step 3200: Training Cost = 0.94222, Validation Cost = 0.90630\n",
            "Step 3300: Training Cost = 0.84902, Validation Cost = 0.89150\n",
            "Step 3400: Training Cost = 0.85372, Validation Cost = 0.87725\n",
            "Step 3500: Training Cost = 0.98458, Validation Cost = 0.86420\n",
            "Step 3600: Training Cost = 0.93550, Validation Cost = 0.85278\n",
            "Step 3700: Training Cost = 0.87493, Validation Cost = 0.83804\n",
            "Step 3800: Training Cost = 0.85323, Validation Cost = 0.82752\n",
            "Step 3900: Training Cost = 0.78484, Validation Cost = 0.81474\n",
            "Step 4000: Training Cost = 0.82393, Validation Cost = 0.80288\n",
            "Step 4100: Training Cost = 0.79494, Validation Cost = 0.79115\n",
            "Step 4200: Training Cost = 0.74908, Validation Cost = 0.78158\n",
            "Step 4300: Training Cost = 0.71264, Validation Cost = 0.77057\n",
            "Step 4400: Training Cost = 0.79674, Validation Cost = 0.75920\n",
            "Step 4500: Training Cost = 0.77823, Validation Cost = 0.74891\n",
            "Step 4600: Training Cost = 0.77208, Validation Cost = 0.73945\n",
            "Step 4700: Training Cost = 0.68351, Validation Cost = 0.72740\n",
            "Step 4800: Training Cost = 0.81895, Validation Cost = 0.71817\n",
            "Step 4900: Training Cost = 0.67383, Validation Cost = 0.71020\n",
            "Step 5000: Training Cost = 0.69374, Validation Cost = 0.69908\n",
            "Step 5100: Training Cost = 0.67426, Validation Cost = 0.69181\n",
            "Step 5200: Training Cost = 0.69856, Validation Cost = 0.67956\n",
            "Step 5300: Training Cost = 0.71322, Validation Cost = 0.67482\n",
            "Step 5400: Training Cost = 0.63028, Validation Cost = 0.66550\n",
            "Step 5500: Training Cost = 0.67703, Validation Cost = 0.65983\n",
            "Step 5600: Training Cost = 0.67314, Validation Cost = 0.64936\n",
            "Step 5700: Training Cost = 0.61548, Validation Cost = 0.64287\n",
            "Step 5800: Training Cost = 0.67509, Validation Cost = 0.63786\n",
            "Step 5900: Training Cost = 0.66126, Validation Cost = 0.62761\n",
            "Step 6000: Training Cost = 0.62100, Validation Cost = 0.62408\n",
            "Step 6100: Training Cost = 0.63368, Validation Cost = 0.61343\n",
            "Step 6200: Training Cost = 0.63878, Validation Cost = 0.60799\n",
            "Step 6300: Training Cost = 0.59919, Validation Cost = 0.60007\n",
            "Step 6400: Training Cost = 0.55799, Validation Cost = 0.59441\n",
            "Step 6500: Training Cost = 0.60166, Validation Cost = 0.58728\n",
            "Step 6600: Training Cost = 0.62769, Validation Cost = 0.58170\n",
            "Step 6700: Training Cost = 0.58058, Validation Cost = 0.57265\n",
            "Step 6800: Training Cost = 0.55985, Validation Cost = 0.56949\n",
            "Step 6900: Training Cost = 0.63061, Validation Cost = 0.56105\n",
            "Step 7000: Training Cost = 0.53216, Validation Cost = 0.55793\n",
            "Step 7100: Training Cost = 0.60711, Validation Cost = 0.54816\n",
            "Step 7200: Training Cost = 0.56629, Validation Cost = 0.54367\n",
            "Step 7300: Training Cost = 0.56000, Validation Cost = 0.53698\n",
            "Step 7400: Training Cost = 0.56756, Validation Cost = 0.53002\n",
            "Step 7500: Training Cost = 0.51800, Validation Cost = 0.52515\n",
            "Step 7600: Training Cost = 0.53163, Validation Cost = 0.51563\n",
            "Step 7700: Training Cost = 0.59149, Validation Cost = 0.51007\n",
            "Step 7800: Training Cost = 0.52666, Validation Cost = 0.50286\n",
            "Step 7900: Training Cost = 0.46809, Validation Cost = 0.49898\n",
            "Step 8000: Training Cost = 0.47918, Validation Cost = 0.49259\n",
            "Step 8100: Training Cost = 0.49713, Validation Cost = 0.48574\n",
            "Step 8200: Training Cost = 0.59006, Validation Cost = 0.47699\n",
            "Step 8300: Training Cost = 0.55329, Validation Cost = 0.47457\n",
            "Step 8400: Training Cost = 0.53704, Validation Cost = 0.46592\n",
            "Step 8500: Training Cost = 0.47094, Validation Cost = 0.46278\n",
            "Step 8600: Training Cost = 0.42805, Validation Cost = 0.45643\n",
            "Step 8700: Training Cost = 0.46734, Validation Cost = 0.45202\n",
            "Step 8800: Training Cost = 0.45856, Validation Cost = 0.44713\n",
            "Step 8900: Training Cost = 0.45748, Validation Cost = 0.44230\n",
            "Step 9000: Training Cost = 0.41836, Validation Cost = 0.44265\n",
            "Step 9100: Training Cost = 0.49796, Validation Cost = 0.43419\n",
            "Step 9200: Training Cost = 0.50369, Validation Cost = 0.43230\n",
            "Step 9300: Training Cost = 0.47534, Validation Cost = 0.42619\n",
            "Step 9400: Training Cost = 0.42526, Validation Cost = 0.42635\n",
            "Step 9500: Training Cost = 0.54234, Validation Cost = 0.41927\n",
            "Step 9600: Training Cost = 0.45174, Validation Cost = 0.41848\n",
            "Step 9700: Training Cost = 0.44495, Validation Cost = 0.41119\n",
            "Step 9800: Training Cost = 0.43986, Validation Cost = 0.40994\n",
            "Step 9900: Training Cost = 0.44599, Validation Cost = 0.40347\n",
            "Step 10000: Training Cost = 0.46448, Validation Cost = 0.40240\n",
            "Step 10100: Training Cost = 0.40309, Validation Cost = 0.39921\n",
            "Step 10200: Training Cost = 0.45046, Validation Cost = 0.39748\n",
            "Step 10300: Training Cost = 0.45484, Validation Cost = 0.39137\n",
            "Step 10400: Training Cost = 0.40308, Validation Cost = 0.39068\n",
            "Step 10500: Training Cost = 0.48308, Validation Cost = 0.39214\n",
            "Step 10600: Training Cost = 0.47681, Validation Cost = 0.38594\n",
            "Step 10700: Training Cost = 0.42644, Validation Cost = 0.38363\n",
            "Step 10800: Training Cost = 0.42373, Validation Cost = 0.37974\n",
            "Step 10900: Training Cost = 0.48033, Validation Cost = 0.37920\n",
            "Step 11000: Training Cost = 0.46037, Validation Cost = 0.37379\n",
            "Step 11100: Training Cost = 0.40370, Validation Cost = 0.37312\n",
            "Step 11200: Training Cost = 0.40976, Validation Cost = 0.36962\n",
            "Step 11300: Training Cost = 0.48853, Validation Cost = 0.36947\n",
            "Step 11400: Training Cost = 0.41583, Validation Cost = 0.36501\n",
            "Step 11500: Training Cost = 0.44129, Validation Cost = 0.36526\n",
            "Step 11600: Training Cost = 0.49674, Validation Cost = 0.36118\n",
            "Step 11700: Training Cost = 0.44461, Validation Cost = 0.36328\n",
            "Step 11800: Training Cost = 0.46859, Validation Cost = 0.35667\n",
            "Step 11900: Training Cost = 0.42687, Validation Cost = 0.35702\n",
            "Step 12000: Training Cost = 0.42826, Validation Cost = 0.35520\n",
            "Step 12100: Training Cost = 0.46012, Validation Cost = 0.35437\n",
            "Step 12200: Training Cost = 0.44495, Validation Cost = 0.35195\n",
            "Step 12300: Training Cost = 0.45967, Validation Cost = 0.35044\n",
            "Step 12400: Training Cost = 0.50986, Validation Cost = 0.34851\n",
            "Step 12500: Training Cost = 0.43596, Validation Cost = 0.34578\n",
            "Step 12600: Training Cost = 0.40440, Validation Cost = 0.34566\n",
            "Step 12700: Training Cost = 0.40903, Validation Cost = 0.34225\n",
            "Step 12800: Training Cost = 0.43082, Validation Cost = 0.34151\n",
            "Step 12900: Training Cost = 0.51574, Validation Cost = 0.33986\n",
            "Step 13000: Training Cost = 0.54073, Validation Cost = 0.34301\n",
            "Step 13100: Training Cost = 0.48954, Validation Cost = 0.33659\n",
            "Step 13200: Training Cost = 0.43111, Validation Cost = 0.33887\n",
            "Step 13300: Training Cost = 0.41067, Validation Cost = 0.33541\n",
            "Step 13400: Training Cost = 0.44786, Validation Cost = 0.33442\n",
            "Step 13500: Training Cost = 0.42897, Validation Cost = 0.33194\n",
            "Step 13600: Training Cost = 0.46196, Validation Cost = 0.33068\n",
            "Step 13700: Training Cost = 0.42617, Validation Cost = 0.33526\n",
            "Step 13800: Training Cost = 0.50299, Validation Cost = 0.33017\n",
            "Step 13900: Training Cost = 0.50171, Validation Cost = 0.32816\n",
            "Step 14000: Training Cost = 0.47811, Validation Cost = 0.32636\n",
            "Step 14100: Training Cost = 0.55985, Validation Cost = 0.32642\n",
            "Early stopping activated.\n",
            "Final Training Cost: 0.559853803834573\n",
            "Final Validation Cost: 0.32641749218004656\n",
            "Accuracy on validation: 0.9234166666666667\n",
            "Epoch: 74, Iterations: 14100\n",
            "\n",
            "Average Magnitude of Weights (with L2):\n",
            "This run is with batch normalization. (use_bnorm=True)\n",
            "Layer 1: Average Magnitude = 0.95965\n",
            "Layer 2: Average Magnitude = 3.11058\n",
            "Layer 3: Average Magnitude = 1.89595\n",
            "Layer 4: Average Magnitude = 1.10362\n",
            "Accuarcy on train:  0.9304666666666667\n",
            "Accuarcy on test:  0.9246\n",
            "The time it takes: 62.01 seconds\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}